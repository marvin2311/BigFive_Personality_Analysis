{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen und erste Betrachtung des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#loading data\n",
    "data = pd.read_csv(\"data-final.csv\", sep=\"\\t\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns.tolist()\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untersuchung + Bereinigen fehlende Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking for missing values\n",
    "data[columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a copy of the dataset and drop the missing values\n",
    "df = data.copy()\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing the length of both datasets\n",
    "\n",
    "print(len(data))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result of the comparison: nearly 3000 objects were dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if there are any missing values left\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einteilung der Aussagen in ihre Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group questions with column name \n",
    "\n",
    "ext_questions = {'EXT1' : 'I am the life of the party',\n",
    "                 'EXT2' : 'I dont talk a lot',\n",
    "                 'EXT3' : 'I feel comfortable around people',\n",
    "                 'EXT4' : 'I keep in the background',\n",
    "                 'EXT5' : 'I start conversations',\n",
    "                 'EXT6' : 'I have little to say',\n",
    "                 'EXT7' : 'I talk to a lot of different people at parties',\n",
    "                 'EXT8' : 'I dont like to draw attention to myself',\n",
    "                 'EXT9' : 'I dont mind being the center of attention',\n",
    "                 'EXT10': 'I am quiet around strangers'}\n",
    "\n",
    "est_questions = {'EST1' : 'I get stressed out easily',\n",
    "                 'EST2' : 'I am relaxed most of the time',\n",
    "                 'EST3' : 'I worry about things',\n",
    "                 'EST4' : 'I seldom feel blue',\n",
    "                 'EST5' : 'I am easily disturbed',\n",
    "                 'EST6' : 'I get upset easily',\n",
    "                 'EST7' : 'I change my mood a lot',\n",
    "                 'EST8' : 'I have frequent mood swings',\n",
    "                 'EST9' : 'I get irritated easily',\n",
    "                 'EST10': 'I often feel blue'}\n",
    "\n",
    "agr_questions = {'AGR1' : 'I feel little concern for others',\n",
    "                 'AGR2' : 'I am interested in people',\n",
    "                 'AGR3' : 'I insult people',\n",
    "                 'AGR4' : 'I sympathize with others feelings',\n",
    "                 'AGR5' : 'I am not interested in other peoples problems',\n",
    "                 'AGR6' : 'I have a soft heart',\n",
    "                 'AGR7' : 'I am not really interested in others',\n",
    "                 'AGR8' : 'I take time out for others',\n",
    "                 'AGR9' : 'I feel others emotions',\n",
    "                 'AGR10': 'I make people feel at ease'}\n",
    "\n",
    "csn_questions = {'CSN1' : 'I am always prepared',\n",
    "                 'CSN2' : 'I leave my belongings around',\n",
    "                 'CSN3' : 'I pay attention to details',\n",
    "                 'CSN4' : 'I make a mess of things',\n",
    "                 'CSN5' : 'I get chores done right away',\n",
    "                 'CSN6' : 'I often forget to put things back in their proper place',\n",
    "                 'CSN7' : 'I like order',\n",
    "                 'CSN8' : 'I shirk my duties',\n",
    "                 'CSN9' : 'I follow a schedule',\n",
    "                 'CSN10' : 'I am exacting in my work'}\n",
    "\n",
    "opn_questions = {'OPN1' : 'I have a rich vocabulary',\n",
    "                 'OPN2' : 'I have difficulty understanding abstract ideas',\n",
    "                 'OPN3' : 'I have a vivid imagination',\n",
    "                 'OPN4' : 'I am not interested in abstract ideas',\n",
    "                 'OPN5' : 'I have excellent ideas',\n",
    "                 'OPN6' : 'I do not have a good imagination',\n",
    "                 'OPN7' : 'I am quick to understand things',\n",
    "                 'OPN8' : 'I use difficult words',\n",
    "                 'OPN9' : 'I spend time reflecting on things',\n",
    "                 'OPN10': 'I am full of ideas'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group columns for every type\n",
    "\n",
    "EXT_col = [col for col in data if col.startswith('EXT')]\n",
    "EST_col = [col for col in data if col.startswith('EST')]\n",
    "AGR_col = [col for col in data if col.startswith('AGR')]\n",
    "CSN_col = [col for col in data if col.startswith('CSN')]\n",
    "OPN_col = [col for col in data if col.startswith('OPN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group positively and negatively keyed questions\n",
    "\n",
    "positiv_questions = ['EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9',\n",
    "                    'EST1', 'EST3', 'EST5', 'EST6', 'EST7', \n",
    "                    'EST8', 'EST9', 'EST10',\n",
    "                    'AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10',\n",
    "                    'CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', \n",
    "                    'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', \n",
    "                    'OPN10']\n",
    "\n",
    "negative_questions = ['EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10',\n",
    "                    'EST2', 'EST4',\n",
    "                    'AGR1', 'AGR3', 'AGR5', 'AGR7', \n",
    "                    'CSN2', 'CSN4', 'CSN6', 'CSN8', \n",
    "                    'OPN2', 'OPN4', 'OPN6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#reverse values of negative questions\n",
    "\n",
    "df.loc[:, negative_questions] = 6 - df.loc[:, negative_questions]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# new data frame that just includes the answers on the questions\n",
    "df_model = df[df.columns.tolist()[:50]]\n",
    "\n",
    "# creating a model and fitting it with the data \n",
    "kmeans = KMeans(n_clusters=5)\n",
    "km_fitted = kmeans.fit(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the column cluster to the data frame\n",
    "\n",
    "df_model.loc[:, 'Cluster'] = km_fitted.labels_\n",
    "df_model.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the rows for every cluster\n",
    "\n",
    "df_model.Cluster.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the amount in a countplot\n",
    "\n",
    "sns.countplot(data=df_model, x='Cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_model['Country_cat'] = df['country']\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(df_model['Country_cat'])\n",
    "df_model.loc[:, 'Country_num'] = le.transform(df_model['Country_cat'])\n",
    "\n",
    "df_model.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionsreduktion & Vergleich der angefertigten Clustern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new data frame to reduce the dimensions for every catagory\n",
    "col_list = list(df_model)\n",
    "ext = col_list[0:10]\n",
    "est = col_list[10:20]\n",
    "agr = col_list[20:30]\n",
    "csn = col_list[30:40]\n",
    "opn = col_list[40:50]\n",
    "\n",
    "df_sums = pd.DataFrame()\n",
    "df_sums['extroversion'] = df_model[ext].sum(axis=1)/10\n",
    "df_sums['neurotic'] = df_model[est].sum(axis=1)/10\n",
    "df_sums['agreeable'] = df_model[agr].sum(axis=1)/10\n",
    "df_sums['conscientious'] = df_model[csn].sum(axis=1)/10\n",
    "df_sums['open'] = df_model[opn].sum(axis=1)/10\n",
    "df_sums['cluster'] = df_model['Cluster'] #Target-Größe\n",
    "\n",
    "# displaying the mean for every catagory in each cluster\n",
    "df_sums.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 0 sehr introvertiert (wenig extrovertiert) und neurotisch --> leichte negative Korrelation\n",
    "\n",
    "Wirkliche Basistypen nicht rauszufiltern, recht ähnlich bei Open - auch sonst alles innerhalb von 1.0 Abweichung nur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms for every category\n",
    "\n",
    "for col in df_sums.columns:\n",
    "    if col == 'cluster':\n",
    "        continue\n",
    "    print(col.upper())\n",
    "    sns.distplot(df_sums[col])\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating heatmaps to visualize the correlations between each category (using 3 different methods)\n",
    "\n",
    "meths = ['pearson', 'spearman', 'kendall']\n",
    "\n",
    "for meth in meths:\n",
    "    print(meth.upper())\n",
    "    corrm= df_sums.drop(columns='cluster').corr(method=meth)\n",
    "    sns.heatmap(corrm, annot=True)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the total time answering the questions (in minutes) \n",
    "df_sums['total_time_min'] = round(df[df.columns.tolist()[50:100]].sum(axis=1)/60000, 2)\n",
    "df_sums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting pairplots of each catagory (be aware that the calculation takes a lot of time!!!)\n",
    "\n",
    "sns.pairplot(df_sums.drop(columns='total_time_min'), hue=\"cluster\", palette=\"bright\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis:  Do the subjects' statements depend on the time of day?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da die Studie in den USA erhoben wurde und die dortige Uhrzeit für alle Probanden übernommen wurde, reduziert sich die Analyse auf US-Bürger. Dazu wird ein Datensatz nur mit US-Amerikanern benutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to get only US citizens and checking the length\n",
    "US = df['country'] == 'US'\n",
    "Dataset_USA = df[US]\n",
    "Dataset_USA\n",
    "#Dataset_USA = Dataset_USA[Dataset_USA.columns.tolist()[:50]]\n",
    "#Dataset_USA['dateload'] = df['dateload']\n",
    "\n",
    "#print(len(Dataset_USA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a column for the time of the day, the survey was uploaded\n",
    "\n",
    "Dataset_USA['time'] = Dataset_USA.dateload.str[11:13].astype(int)\n",
    "Dataset_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the quantity of every hour\n",
    "sns.countplot(data=Dataset_USA, x=Dataset_USA['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering the new time column in the categories 'morning', 'noon', 'evening', 'night'\n",
    "\n",
    "\n",
    "intervals = [0,6,12,18,24]\n",
    "daytime_categories = ['morning', 'noon', 'evening', 'night']\n",
    "\n",
    "Dataset_USA['daytime'] = pd.cut(x= Dataset_USA['time'], bins = intervals, labels = daytime_categories, include_lowest = True)\n",
    "Dataset_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the different categories in daytime\n",
    "sns.countplot(data = Dataset_USA, x = Dataset_USA['daytime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating variables for each category in daytime\n",
    "morning = Dataset_USA['daytime'] == 'morning'\n",
    "noon = Dataset_USA['daytime'] == 'noon'\n",
    "evening = Dataset_USA['daytime'] == 'evening'\n",
    "night = Dataset_USA['daytime'] == 'night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking up the dataset for the morning category\n",
    "Dataset_USA[morning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#looking up the mean values of morning category\n",
    "Dataset_USA[morning].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the mean value of every daytime category in a list\n",
    "mean_list_morning = list(Dataset_USA[morning].mean())\n",
    "mean_list_noon = list(Dataset_USA[noon].mean())\n",
    "mean_list_evening = list(Dataset_USA[evening].mean())\n",
    "mean_list_night = list(Dataset_USA[night].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating a new USA data frame to reduce the dimensions and compare the answers in relation to the daytime\n",
    "col_list = list(Dataset_USA)\n",
    "ext = col_list[0:10]\n",
    "est = col_list[10:20]\n",
    "agr = col_list[20:30]\n",
    "csn = col_list[30:40]\n",
    "opn = col_list[40:50]\n",
    "\n",
    "df_sums_usa = pd.DataFrame()\n",
    "df_sums_usa['extroversion'] = Dataset_USA[ext].sum(axis=1)/10\n",
    "df_sums_usa['neurotic'] = Dataset_USA[est].sum(axis=1)/10\n",
    "df_sums_usa['agreeable'] = Dataset_USA[agr].sum(axis=1)/10\n",
    "df_sums_usa['conscientious'] = Dataset_USA[csn].sum(axis=1)/10\n",
    "df_sums_usa['open'] = Dataset_USA[opn].sum(axis=1)/10\n",
    "df_sums_usa['daytime'] = Dataset_USA['daytime']\n",
    "\n",
    "# displaying the mean for every catagory in each cluster\n",
    "df_sums_usa.groupby('daytime').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**: The answers of the americans who participated in the survey do not differ on different daytimes! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the personalities around the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_countries = pd.DataFrame(df['country'].value_counts())\n",
    "representive_countries = count_countries[count_countries['country'] >= 1000]\n",
    "representive_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a newdata frame to reduce the dimensions and compare the statements of the countrys\n",
    "col_list = list(df)\n",
    "ext = col_list[0:10]\n",
    "est = col_list[10:20]\n",
    "agr = col_list[20:30]\n",
    "csn = col_list[30:40]\n",
    "opn = col_list[40:50]\n",
    "\n",
    "df_sums_world = pd.DataFrame()\n",
    "df_sums_world['extroversion'] = df[ext].sum(axis=1)/10\n",
    "df_sums_world['neurotic'] = df[est].sum(axis=1)/10\n",
    "df_sums_world['agreeable'] = df[agr].sum(axis=1)/10\n",
    "df_sums_world['conscientious'] = df[csn].sum(axis=1)/10\n",
    "df_sums_world['open'] = df[opn].sum(axis=1)/10\n",
    "df_sums_world['country'] = df['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all rows from df_sums_world that have les than 1000 participants --> takes very long!!\n",
    "for country in df_sums_world['country']:\n",
    "    if country in representive_countries:\n",
    "        pass\n",
    "    else:\n",
    "        df_sums_world = df_sums_world.drop(df_sums_world[(df_sums_world.country == country)].index)\n",
    "\n",
    "df_sums_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the mean for every catagory in each country\n",
    "country_means_per_category = df_sums_world.groupby('country').mean() \n",
    "country_means_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting an overview about the range/highs/lows in the columns and examine the countries with max-min values\n",
    "print(country_means_per_category.loc[country_means_per_category['extroversion'].idxmax()])\n",
    "print()\n",
    "print(country_means_per_category.loc[country_means_per_category['extroversion'].idxmin()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most extroversion:**\n",
    "\n",
    "**Most interversion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(country_means_per_category.loc[country_means_per_category['neurotic'].idxmax()])\n",
    "print(country_means_per_category.loc[country_means_per_category['neurotic'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most neurotic:**\n",
    "\n",
    "**least neurotic :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(country_means_per_category.loc[country_means_per_category['agreeable'].idxmax()])\n",
    "print(country_means_per_category.loc[country_means_per_category['agreeable'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most agreeable:**\n",
    "\n",
    "**least agreeable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_means_per_category.loc[country_means_per_category['conscientious'].idxmax()])\n",
    "print(country_means_per_category.loc[country_means_per_category['conscientious'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most conscientious:**\n",
    "\n",
    "**least conscientious:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(country_means_per_category.loc[country_means_per_category['open'].idxmax()])\n",
    "print(country_means_per_category.loc[country_means_per_category['open'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most open:**\n",
    "\n",
    "**least open:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot countries with more than 10.000 participants\n",
    "\n",
    "countries = pd.DataFrame(df['country'].value_counts())\n",
    "countries_10k = countries[countries['country'] >= 10000]\n",
    "\n",
    "sns.barplot(data=countries_10k, x=countries_10k.index, y='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking at the means of germany\n",
    "countries = pd.DataFrame(df['country'].value_counts())\n",
    "countries_10k = countries[countries['country'] >= 10000]\n",
    "\n",
    "germany = df_sums_world['country'] == 'DE'\n",
    "df_sums_world[germany].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deutschland sehr ausgeglichene Nation --> Überall circa im Durchschnitt, etwas weniger gewissenhaft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mean comparison of those countries that had more than 10.000 participants\n",
    "country_means_per_category[df['country'].value_counts() >= 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating heatmaps to visualize the correlations between each category (using 3 different methods)\n",
    "\n",
    "meths = ['pearson', 'spearman', 'kendall']\n",
    "\n",
    "for meth in meths:\n",
    "    print(meth.upper())\n",
    "    corrm= country_means_per_category[df['country'].value_counts() >= 10000].corr(method=meth)\n",
    "    sns.heatmap(corrm, annot=True)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** There are only narrow differences between the mean answers of the examined countries (10.000+ participants). Therefore, also the nationality of the participants is not critical for the given statements.\n",
    "\n",
    "Interesting could be a view on the correlation matrix. In the smaller Dataset country_mean_per_country are many strong (positive as well as negative) correlations betwenn the columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of every participant in the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot location of people who attend the survey\n",
    "\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# get coordinates\n",
    "\n",
    "lat_cor = pd.to_numeric(df['lat_appx_lots_of_err'], errors='coerce')\n",
    "long_cor = pd.to_numeric(df['long_appx_lots_of_err'], errors='coerce')\n",
    "\n",
    "# transform coordinates to use them with geopandas, load geopandas world map\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(long_cor, lat_cor)]\n",
    "gdf = GeoDataFrame(df, geometry=geometry)\n",
    "geo_file = gpd.datasets.get_path('naturalearth_lowres')\n",
    "\n",
    "# create the plot \n",
    "\n",
    "world = gpd.read_file(geo_file)\n",
    "world_ax = world.drop(159).plot(figsize=(60, 10))\n",
    "gdf.plot(ax=world_ax, marker='x', color='green', markersize=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis political bias and regions with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import reverse_geocoder as rg \n",
    "\n",
    "Dataset_USA = Dataset_USA.rename(columns = {\"lat_appx_lots_of_err\":\"lat_appx\", \"long_appx_lots_of_err\":\"long_appx\"})\n",
    "Dataset_USA.head()\n",
    "\n",
    "latitudesUS = Dataset_USA['lat_appx'].tolist()\n",
    "longitudesUS = Dataset_USA['long_appx'].tolist()\n",
    "\n",
    "coordinates = zip(latitudesUS,longitudesUS)\n",
    "\n",
    "coordinates = list(coordinates)\n",
    "\n",
    "coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reverseGeocode(x):\n",
    "    result = rg.search(x)\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for state in reverseGeocode(coordinates):\n",
    "    states.append(state['admin1'])\n",
    "    \n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_USA['state'] = states\n",
    "Dataset_USA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_longitudes = Dataset_USA['long_appx'].astype(float).abs()\n",
    "latitudes_for_regionsplit = latitudesUS = Dataset_USA['lat_appx'].tolist()\n",
    "\n",
    "coordinates = zip(latitudes_for_regionsplit, positive_longitudes)\n",
    "\n",
    "regions = []\n",
    "\n",
    "for c in coordinates:\n",
    "    if float(c[0]) < 36 and 80 < float(c[1]) < 110:\n",
    "        regions.append('southern state')\n",
    "    elif float(c[0]) > 40 and 80 < float(c[1]) < 120:\n",
    "        regions.append('northern state')\n",
    "    elif float(c[1]) > 83:\n",
    "        regions.append('east coast')\n",
    "    elif float(c[1]) > 117:\n",
    "        regions.append('west coast')\n",
    "    else:\n",
    "        regions.append('landlocked')\n",
    "        \n",
    "        \n",
    "regions\n",
    "           \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_USA['region'] = regions\n",
    "Dataset_USA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Dataset_USA['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_USA.drop(Dataset_USA.loc[Dataset_USA['state']== 'Baja California'].index, inplace=True)\n",
    "Dataset_USA.drop(Dataset_USA.loc[Dataset_USA['state']== 'New Brunswick'].index, inplace=True)\n",
    "Dataset_USA.drop(Dataset_USA.loc[Dataset_USA['state']== 'British Columbia'].index, inplace=True)\n",
    "Dataset_USA.drop(Dataset_USA.loc[Dataset_USA['state']== 'Manitoba'].index, inplace=True)\n",
    "Dataset_USA.drop(Dataset_USA.loc[Dataset_USA['state']== 'Western'].index, inplace=True)\n",
    "Dataset_USA['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "democratics = ['California', 'Tennessee', 'New York', 'Maine', 'New Jersey',\n",
    "                    'Missouri', 'Arkansas', 'Connecticut', 'Michigan', 'Minnesota', \n",
    "                    'New Hampshire', 'Ohio', 'Colorado',\n",
    "                    'Wisconsin', 'Nevada', 'Illinois', 'Maryland', 'Georgia', 'Washington, D.C.',\n",
    "                    'Pennsylvania', 'Delaware', 'Hawaii', 'Kentucky', 'Louisiana', 'Oregon', \n",
    "                    'Vermont', 'New Mexico', 'Rhode Island', 'Idaho', 'West Virginia']\n",
    "\n",
    "republicans = ['Florida', 'Virginia', 'Kansas', 'South Carolina', 'Texas',\n",
    "                    'Mississippi', 'North Carolina',\n",
    "                    'Arizona', 'Indiana', 'Oklahoma', 'South Dakota', \n",
    "                    'Alaska', 'Utah', 'Nebraska', 'Wyoming', \n",
    "                    'Baja California', 'North Dakota', 'Montana','Alabama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = Dataset_USA['state']\n",
    "political_bias = []\n",
    "for s in states:\n",
    "    if s in democratics:\n",
    "        political_bias.append('blue')\n",
    "    else:\n",
    "        political_bias.append('red')\n",
    "        \n",
    "#df['Category'] = df['Count']\n",
    "#df.loc[df['state'] > 3, 'political bias'] = 'red'\n",
    "#df.loc[df['state'] > 3, 'political bias'] = 'blue'\n",
    "#df.loc[df['state'] > 3, 'political bias'] = 'yellow'    \n",
    "\n",
    "\n",
    "Dataset_USA['political bias'] = political_bias \n",
    "Dataset_USA.head(10)\n",
    "\n",
    "\n",
    "#not mature yet.. as you can see in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data= Dataset_USA, x = Dataset_USA['state'] )\n",
    "sns.countplot(data= Dataset_USA, x = Dataset_USA['region'] )\n",
    "sns.countplot(data= Dataset_USA, x = Dataset_USA['political bias'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data= Dataset_USA, x = Dataset_USA['region'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "sns.countplot(data= Dataset_USA, x = Dataset_USA['state'], order = Dataset_USA['state'].value_counts().index)\n",
    "\n",
    "plt.xlabel(\"states\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_states = pd.DataFrame(Dataset_USA['state'].value_counts())\n",
    "states_10k = US_states[US_states['state'] >= 7000]\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.barplot(data=states_10k, x=states_10k.index, y='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examine_locations = Dataset_USA[['state','region','political bias']]\n",
    "Examine_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"region\", y=\"EXT1\", hue=\"political bias\", data=Dataset_USA, palette=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"political bias\", y=\"EXT1\", hue=\"region\", data=Dataset_USA, palette=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for integer encoding using sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "# Encoding the variable\n",
    "Dataset_USA_transformed = Dataset_USA.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "\n",
    "Dataset_USA_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inverse the encoded\n",
    "Dataset_USA_reversed = Dataset_USA_transformed.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "Dataset_USA_reversed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_USA.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_USA_predict_bias = Dataset_USA.drop(['EXT1_E',\n",
    " 'EXT2_E',\n",
    " 'EXT3_E',\n",
    " 'EXT4_E',\n",
    " 'EXT5_E',\n",
    " 'EXT6_E',\n",
    " 'EXT7_E',\n",
    " 'EXT8_E',\n",
    " 'EXT9_E',\n",
    " 'EXT10_E',\n",
    " 'EST1_E',\n",
    " 'EST2_E',\n",
    " 'EST3_E',\n",
    " 'EST4_E',\n",
    " 'EST5_E',\n",
    " 'EST6_E',\n",
    " 'EST7_E',\n",
    " 'EST8_E',\n",
    " 'EST9_E',\n",
    " 'EST10_E',\n",
    " 'AGR1_E',\n",
    " 'AGR2_E',\n",
    " 'AGR3_E',\n",
    " 'AGR4_E',\n",
    " 'AGR5_E',\n",
    " 'AGR6_E',\n",
    " 'AGR7_E',\n",
    " 'AGR8_E',\n",
    " 'AGR9_E',\n",
    " 'AGR10_E',\n",
    " 'CSN1_E',\n",
    " 'CSN2_E',\n",
    " 'CSN3_E',\n",
    " 'CSN4_E',\n",
    " 'CSN5_E',\n",
    " 'CSN6_E',\n",
    " 'CSN7_E',\n",
    " 'CSN8_E',\n",
    " 'CSN9_E',\n",
    " 'CSN10_E',\n",
    " 'OPN1_E',\n",
    " 'OPN2_E',\n",
    " 'OPN3_E',\n",
    " 'OPN4_E',\n",
    " 'OPN5_E',\n",
    " 'OPN6_E',\n",
    " 'OPN7_E',\n",
    " 'OPN8_E',\n",
    " 'OPN9_E',\n",
    " 'OPN10_E','dateload',\n",
    " 'screenw',\n",
    " 'screenh',\n",
    " 'introelapse',\n",
    " 'testelapse',\n",
    " 'endelapse',\n",
    " 'IPC',\n",
    " 'country',\n",
    " 'lat_appx',\n",
    " 'long_appx',\n",
    " 'time',\n",
    " 'daytime',\n",
    " 'state',\n",
    " 'region',], axis = 1)\n",
    "\n",
    "Dataset_USA_predict_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = Dataset_USA_predict_bias.drop(columns=['political bias'])\n",
    "y = Dataset_USA_predict_bias['political bias']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state = 0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "decision_tree_clf = DecisionTreeClassifier(max_depth = 6, random_state = 0)\n",
    "decision_tree_clf.fit(X_train,y_train)\n",
    "\n",
    "y_prediction = decision_tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = decision_tree_clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best depth of the Decision tree?\n",
    "fen = [s.split('(')[0].strip() for s in X]\n",
    "\n",
    "\n",
    "# List of values to try for max_depth:\n",
    "max_depth_range = list(range(1, 6))\n",
    "accuracy = []\n",
    "print('depth, acc. Feat.imp.', fen)\n",
    "for depth in max_depth_range:\n",
    "    dt_clf = DecisionTreeClassifier(max_depth = depth, \n",
    "                             random_state = 0)\n",
    "    dt_clf.fit(X_train, y_train)    \n",
    "    score = dt_clf.score(X_test, y_test)\n",
    "    print(depth, score, np.round(dt_clf.feature_importances_,3))\n",
    "    accuracy.append(score)\n",
    "\n",
    "print()\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks([1,2,3,4,5])\n",
    "plt.grid(True)\n",
    "plt.ylim([0,1])\n",
    "plt.plot(max_depth_range, accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which features are the most important ones?\n",
    "importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(decision_tree_clf.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False)\n",
    "importances\n",
    "\n",
    "# --> OPN7 0,2 importance, most important question by far: Interpretation?\n",
    "# 15 questions with 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "text_representation = tree.export_text(decision_tree_clf)\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8), dpi=100)\n",
    "tree.plot_tree(decision_tree_clf, feature_names= X, class_names= 'political bias', filled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "# DOT data\n",
    "dot_data = tree.export_graphviz(decision_tree_clf, out_file=None, \n",
    "                                feature_names= X_train,  \n",
    "                                class_names='political bias',\n",
    "                                rounded=True,\n",
    "                                filled=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graphviz.Source(dot_data, format=\"png\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomforest_classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy')\n",
    "randomforest_classifier.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_train = randomforest_classifier.predict(X_train)\n",
    "print(classification_report(y_train, y_predict_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, y_predict_train)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "print(confusion_matrix(y_train, y_predict_train))   \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_train, y_predict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = randomforest_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "print(confusion_matrix(y_test, y_predict_test))   \n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import corrcoef\n",
    "    \n",
    "    #boston.CRIM.to_numpy()    # As a NumPy array\n",
    "corrcoef(Dataset_USA_transformed.EXT1.to_numpy(), Dataset_USA_transformed['political bias'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import corrcoef\n",
    "    \n",
    "    #boston.CRIM.to_numpy()    # As a NumPy array\n",
    "corrcoef(Dataset_USA_transformed.EXT1.to_numpy(), Dataset_USA_transformed.region.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import corrcoef\n",
    "    \n",
    "    #boston.CRIM.to_numpy()    # As a NumPy array\n",
    "corrcoef(Dataset_USA_transformed.region.to_numpy(), Dataset_USA_transformed['political bias'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
